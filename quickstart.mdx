---
title: "Quickstart"
description: "This guide will walk you through installing Agensight and capturing your first LLM trace — both via manual SDK tracing and automatic prompt/tool/agent detection using the MCP server."
---

## Step 1: Set Up Agensight MCP Server (Required)

The MCP server helps extract agents, tools, and prompts from your codebase — it's required for Agensight to function.

### Installation

```bash
# Clone the MCP server repo
git clone git@github.com:PYPE-AI-MAIN/agensight_mcpserver.git
cd agensight_mcpserver

# Create a virtual environment
python -m venv mcp-env
source mcp-env/bin/activate  # On Windows: mcp-env\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

---

## Step 2: Generate `agensight.config.json`

You have **two options** to create your config file:

### Option A: With Cursor (Automatic)

Add this to `~/.cursor/mcp.json`:

```json
{
  "mcpServers": {
    "agensight-server": {
      "command": "/path/to/agensight_mcpserver/mcp-env/bin/python",
      "args": ["/path/to/agensight_mcpserver/server.py"],
      "description": "Tool to generate agensight config"
    }
  }
}
```

> Replace `/path/to/agensight_mcpserver` with your actual path.

In Cursor, ask:

```
Please analyze this codebase using the generateAgensightConfig MCP tool
```

This will generate a valid `agensight.config.json` file for your project.

### Option B: Without Cursor (Manual)

If you're not using Cursor, you can manually create a `agensight.config.json` file. The format looks like this:

```json
{
  "projectName": "my-llm-app",
  "agents": ["Agent1", "Agent2"],
  "tools": ["ToolA", "ToolB"],
  "prompts": ["prompt_id_1", "prompt_id_2"]
}
```

## Step 3: Install Agensight SDK and Capture a Trace

We recommend using a virtual environment for your SDK-based app:

```bash
# Create and activate virtual env
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install the SDK
pip install agensight
```

---

## Step 4: Run Your First Trace

Create a file `my_llm_app.py`:

```python
from agensight import init, trace, span
import openai  # pip install openai

# 1. Initialize Agensight
init(name="my-first-llm-project")

@trace("joke_generation_workflow")
def generate_a_joke():
    @span()
    def call_openai_for_joke():
        return openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Tell me a fun fact about programming"}]
        )

    print("Calling LLM to get a fun fact...")
    response = call_openai_for_joke()
    fun_fact = response.choices[0].message.content
    print(f"Fun Fact: {fun_fact}")
    return fun_fact

if __name__ == "__main__":
    generate_a_joke()
```

Make sure to set your `OPENAI_API_KEY` as an environment variable.

Run the app:

```bash
python my_llm_app.py
```

---

## Step 5: View the Dashboard

After tracing your first call, open the Agensight dashboard:

```bash
agensight view
```

This opens a browser window at `http://localhost:5001` with full trace visualization.

---

## What’s Next?

- **Explore the dashboard:** Token usage, spans, and workflows.
- **Use decorators in your app:** `@trace()` for workflows, `@span()` for steps.
- **Add prompt IDs and agent context to config.**
- [**See Core Concepts**](./core-concepts/features-overview) for a deep dive.