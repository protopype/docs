---
title: 'Agensight Tracing: Core Concepts'
description: 'How to initialize Agensight tracing, define main operations with @trace, and add granular details with @span.'
---

# Agensight Tracing: Core Concepts

This guide covers the essentials for instrumenting your Python application with Agensight:
1.  **Initializing** the tracing system using `init`.
2.  Defining top-level operations or requests as **traces** using the `@trace` decorator.
3.  Instrumenting specific units of work within a trace as **spans** using the `@span` decorator.

## Initializing Agensight Tracing

Before any traces or spans can be created, you must initialize the Agensight tracing system. This is typically done once at the beginning of your application's lifecycle. The function for this is `init`.

**Purpose**:
The `init` function configures the core tracing mechanism, including setting up how and where trace data is exported.

*   **Key Parameters for `init`**:
    *   `service_name` (str, optional): A name for your application or service (e.g., "user-api", "data-pipeline"). Defaults to `"default"`. This helps in organizing and filtering traces.
    *   `exporter_type` (str, optional): Determines the destination for your trace data.
        *   If `None` (default): Agensight checks the `TRACE_EXPORTER` environment variable. If this variable is not set, it defaults to `"console"` (traces are printed to the standard output).
        *   `"db"`: Traces are stored in a local SQLite database (default filename: `traces.db`).
        *   Other values (e.g., `"otlp"`, `"zipkin"`, `"jaeger"`): Can be used if you intend to integrate with other OpenTelemetry-compatible backends. (Requires appropriate OTel exporter packages to be installed).

**Relevant File (Implementation Detail)**: The underlying setup logic is in `agensight/tracing/setup.py`.

**Usage Example:**

```python
from agensight import init

# Example 1: Basic initialization (traces go to console by default)
init(service_name="my-awesome-app")

# Example 2: Initialize to use the database exporter
# Ensure TRACE_EXPORTER=db in your environment, or specify directly:
# init(service_name="my-data-processor", exporter_type="db")

# Your application code follows...
# from agensight import trace, span
# @trace(...)
# def my_function(): ...
```

## Defining Main Operations with `@trace`

The `@trace` decorator is used to mark the boundaries of a significant, self-contained operation or request in your application. Each function call decorated with `@trace` initiates a new, independent trace. This is ideal for entry points like API request handlers, main processing functions in a script, or any high-level task.

**Syntax**: `@trace(name: Optional[str] = None, **default_attributes)`

*   **`name` (str, optional)**: Assigns a custom name to the trace. If not provided, the name of the decorated function (`func.__name__`) is used.
*   **`**default_attributes` (dict, optional)**: A dictionary of key-value pairs that provide metadata for the entire trace. These attributes are especially useful when using the `"db"` exporter, as they get stored with the trace record.

**How it Works**:
When a `@trace`-decorated function is called:
1.  A unique `trace_id` is generated.
2.  The function's execution, including start and end times, is recorded.
3.  If using the `"db"` exporter, details like the trace ID, name, timing, and any `default_attributes` are saved.
4.  Crucially, it sets up a context so that any `@span`-decorated functions called within the execution of this traced function are automatically associated with this parent trace.

**Relevant File (Implementation Detail)**: The `@trace` decorator is defined in `agensight/tracing/decorators.py`.

---

### `@trace` Examples

#### Simple Example: Basic Task Execution

This example shows `@trace` on a simple function representing a single, self-contained task.

```python
from agensight import init, trace
import time

init(service_name="task-runner-service")

@trace(name="execute_report_generation", report_type="daily_summary")
def generate_report(report_id: str):
    print(f"Starting report generation for: {report_id}")
    # Simulate work
    time.sleep(0.5) 
    result = f"Report {report_id} content."
    print(f"Finished report generation for: {report_id}")
    return {"status": "success", "report_id": report_id, "content_length": len(result)}

# When generate_report is called, a new trace named "execute_report_generation" starts.
report_data = generate_report("report_123")
print(report_data)
```

#### Complex Example: Multi-Step Data Pipeline

This example demonstrates `@trace` on a main pipeline function that orchestrates several steps, some of which might be instrumented with `@span` (as shown in `@span` examples later) or could be other traced functions.

```python
from agensight import init, trace, span # span might be used by helpers
import time

init(service_name="data-pipeline-service", exporter_type="console")

# --- Helper functions (could be in other modules) ---
def fetch_raw_data(source_system: str) -> dict:
    print(f"  Fetching data from {source_system}...")
    time.sleep(0.2)
    if source_system == "source_b": # Simulate a potential issue
        return {"data": None, "error": "Connection failed"}
    return {"data": f"raw_data_from_{source_system}", "items_count": 100}

@span(name="data_cleaning_step") # This span will be part of the 'process_pipeline' trace
def clean_data(raw_data_package: dict) -> dict:
    print(f"  Cleaning data...")
    time.sleep(0.3)
    # Simulate cleaning
    cleaned_data = str(raw_data_package.get("data")).replace("raw_", "cleaned_")
    return {"cleaned_data": cleaned_data, "items_processed": raw_data_package.get("items_count")}

def archive_data(data_to_archive: dict, archive_location: str): # Not explicitly spanned here
    print(f"  Archiving data to {archive_location}...")
    time.sleep(0.1)
    print(f"  Data archived successfully.")
# --- End Helper functions ---

@trace(name="customer_data_ingestion_pipeline", pipeline_version="2.1")
def process_pipeline(source: str, archive_target: str):
    print(f"Starting pipeline for source: {source}")
    
    raw_data = fetch_raw_data(source)
    if raw_data.get("error"):
        print(f"Pipeline aborted at fetch stage: {raw_data.get('error')}")
        return {"status": "failed", "stage": "fetch", "reason": raw_data.get("error")}
        
    processed_data = clean_data(raw_data) # 'clean_data' is spanned
    
    archive_data(processed_data, archive_target)
    
    print(f"Pipeline for source {source} completed successfully.")
    return {"status": "success", "result_summary": f"Processed {processed_data.get('items_processed')} items."}

# Each call to process_pipeline will generate a new trace
result1 = process_pipeline(source="source_a", archive_target="/mnt/archive/source_a")
print(f"Pipeline Result 1: {result1}\n")

result2 = process_pipeline(source="source_b", archive_target="/mnt/archive/source_b")
print(f"Pipeline Result 2: {result2}")
```

## Adding Granular Details with `@span`

The `@span` decorator is used to instrument specific units of work or operations *within an existing trace*. Think of spans as children of a trace, providing a more detailed breakdown of how time is spent and what happens during the execution of a traced operation. A function decorated with `@span` should generally be called from within a function that is already decorated with `@trace` or another `@span`.

**Syntax**: `@span(name: Optional[str] = None, metadata: Optional[Dict[str, Any]] = None, input: Optional[Any] = None, output: Optional[Any] = None)`

*   **`name` (str, optional)**: A custom name for this specific span/operation. If omitted, the decorated function's name is used.
*   **`metadata` (Dict[str, Any], optional)**: A dictionary of custom key-value attributes that provide additional context specific to this span.
*   **`input` (Any, optional)**: Allows you to explicitly provide the input data relevant to this span's operation. If not provided, Agensight may attempt to infer it.
*   **`output` (Any, optional)**: Allows you to explicitly provide the output data from this span's operation. If not provided, Agensight may attempt to infer it from the return value.

**How it Works**:
When a `@span`-decorated function is called:
1.  It automatically links to the active `trace_id` established by its parent `@trace` (or parent `@span`).
2.  It records its own `span_name`, any provided `metadata`, and its execution duration.
3.  It attempts to capture input and output data (either explicitly provided via `input`/`output` parameters, or inferred from function arguments/return values). This is particularly useful for understanding data flow and for AI/LLM interactions where input prompts and output completions are critical.
4.  Special handling for LLM interactions: If the function's result contains LLM token usage information (e.g., in a `usage` attribute or a "usage" key in a dictionary result), these details (like `total_tokens`, `prompt_tokens`) are extracted and added as attributes to the span.
5.  If an error occurs during the span's execution, its status is marked appropriately (e.g., `ERROR`).

**Relevant File (Implementation Detail)**: The `@span` decorator is defined in `agensight/tracing/decorators.py`.


### `@span` Examples

#### Simple Example: LLM Call

This is a common use case for `@span` â€“ instrumenting a call to an external service like an LLM.

```python
from agensight import init, trace, span
# import openai # For a real LLM call
import time
import os

# init(service_name="llm-app", exporter_type="console")
# For real OpenAI calls, ensure your API key is set:
# openai.api_key = os.getenv("OPENAI_API_KEY")
# client = openai.OpenAI() 

@span(name="fetch_openai_completion", metadata={"model_provider": "openai"})
def get_llm_response(prompt: str, model_name: str = "gpt-3.5-turbo"):
    print(f"    Calling LLM ({model_name}) for prompt: '{prompt[:30]}...'")
    try:
        # Simulate LLM call
        time.sleep(0.4)
        # For a real call:
        # response = client.chat.completions.create(
        #     model=model_name,
        #     messages=[{"role": "user", "content": prompt}]
        # )
        # simulated_content = response.choices[0].message.content
        # simulated_usage = response.usage # OpenAIObject with token counts
        
        simulated_content = f"Simulated LLM response to: {prompt}"
        simulated_usage = {"prompt_tokens": len(prompt)//4, "completion_tokens": len(simulated_content)//4, "total_tokens": (len(prompt) + len(simulated_content))//4}

        # Agensight will automatically try to pick up 'usage' from the return if it's a dict or object with 'usage'
        return {"content": simulated_content, "usage": simulated_usage, "model_used": model_name}
    except Exception as e:
        print(f"    LLM call failed: {e}")
        # Error will be captured by the span
        raise

@trace(name="generate_article_summary")
def summarize_article_content(article_text: str):
    print(f"  Generating summary for article (length: {len(article_text)})...")
    
    # This call to get_llm_response will create a span under the "generate_article_summary" trace.
    # Input (prompt) and Output (response dict) will be captured by the span.
    # LLM token usage will also be captured from the 'usage' key in the returned dictionary.
    summary_details = get_llm_response(prompt=f"Summarize this article: {article_text}")
    
    print(f"  Summary received: '{summary_details.get('content')[:50]}...'")
    return {"summary": summary_details.get('content'), "tokens_used": summary_details.get("usage").get("total_tokens")}

# Initialize once
init(service_name="content-processor", exporter_type="console")

long_article = "This is a very long article about Agensight tracing..." * 5
summary_result = summarize_article_content(long_article)
print(f"Final Summary Result: {summary_result}")
```

#### Complex Example: Multi-Step User Profile Enhancement

This example shows multiple `@span`-decorated functions being called sequentially within a single trace, where one spanned function might even call another.

```python
from agensight import init, trace, span
import time

init(service_name="user-profile-builder", exporter_type="console")

@span(name="fetch_user_base_data", metadata={"source_db": "user_db_main"})
def fetch_user_from_db(user_id: str) -> dict:
    print(f"    Querying database for user: {user_id}")
    time.sleep(0.1)
    if user_id == "unknown_user":
        raise ValueError(f"User {user_id} not found in database.")
    return {"id": user_id, "name": "Alex Query", "email": f"{user_id}@example.com", "status": "active"}

@span(name="fetch_user_activity_score", metadata={"service_endpoint": "activity-service/score"})
def get_activity_score(user_id: str) -> int:
    print(f"      Fetching activity score for {user_id} from external service...")
    time.sleep(0.05) # Simulate quick API call
    return hash(user_id) % 100 # Simulated score

@span(name="enrich_user_with_activity", metadata={"version": "1.1"})
def enrich_user_profile(user_data: dict) -> dict:
    user_id = user_data.get("id")
    print(f"    Enriching profile for user: {user_id}")
    
    # This call to get_activity_score creates a nested span
    activity_score = get_activity_score(user_id) 
    
    enriched_profile = user_data.copy()
    enriched_profile["activity_score"] = activity_score
    enriched_profile["profile_completeness"] = "75%" if activity_score > 50 else "50%"
    time.sleep(0.1) # Simulate some processing for enrichment
    print(f"    Profile for {user_id} enriched.")
    return enriched_profile

@trace(name="construct_full_user_dashboard_profile", user_segment="premium")
def build_user_dashboard_profile(user_id: str):
    print(f"  Building full dashboard profile for user: {user_id}")
    try:
        # First span: get base data
        base_profile = fetch_user_from_db(user_id)
        
        # Second span: enrich the base data; this span internally calls another spanned function
        full_profile = enrich_user_profile(base_profile)
        
        print(f"  Dashboard profile for {user_id} constructed successfully.")
        return {"status": "success", "profile_data": full_profile}
    except ValueError as ve:
        print(f"  Failed to build profile for {user_id}: {ve}")
        # The error from fetch_user_from_db will be captured by its span,
        # and the trace itself will also reflect the failure.
        return {"status": "error", "reason": str(ve)}

# Test cases
profile1 = build_user_dashboard_profile("alex_doe")
print(f"Profile 1: {profile1}\n")

profile2 = build_user_dashboard_profile("unknown_user") # This will trigger an error
print(f"Profile 2: {profile2}")